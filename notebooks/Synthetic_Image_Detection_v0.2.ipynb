{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlAd5j3R-d5i"
      },
      "source": [
        "# Synthetic Image Detection\n",
        "---\n",
        "\n",
        "### Mk-0.2  :- Basic CNN (Multi-generator training)\n",
        "* Used CNN with...\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Data-preprocessing (GenImage -> BigGAN+VQDM dataset) :-\n",
        "\n",
        "*   Resizing all the images to 224x224 (std. for CNN based models). CNN expects fixed input shape. Avoids inconsistent pixel distributions.\n",
        "*   Resizing was done locally using python + cmd.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Requirments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from -r requirements.txt (line 2)) (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from -r requirements.txt (line 3)) (2.5.1+cu121)\n",
            "Requirement already satisfied: matplotlib in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from -r requirements.txt (line 4)) (3.10.7)\n",
            "Requirement already satisfied: scikit-learn in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from -r requirements.txt (line 5)) (1.7.2)\n",
            "Requirement already satisfied: tqdm in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from -r requirements.txt (line 6)) (4.67.1)\n",
            "Requirement already satisfied: jupyter in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from -r requirements.txt (line 7)) (1.1.1)\n",
            "Requirement already satisfied: filelock in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: networkx in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2025.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: numpy in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from torchvision->-r requirements.txt (line 2)) (2.3.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from torchvision->-r requirements.txt (line 2)) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (25.0)\n",
            "Requirement already satisfied: pyparsing>=3 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n",
            "Requirement already satisfied: colorama in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from tqdm->-r requirements.txt (line 6)) (0.4.6)\n",
            "Requirement already satisfied: notebook in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter->-r requirements.txt (line 7)) (7.4.7)\n",
            "Requirement already satisfied: jupyter-console in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter->-r requirements.txt (line 7)) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter->-r requirements.txt (line 7)) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter->-r requirements.txt (line 7)) (7.0.1)\n",
            "Requirement already satisfied: ipywidgets in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter->-r requirements.txt (line 7)) (8.1.7)\n",
            "Requirement already satisfied: jupyterlab in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter->-r requirements.txt (line 7)) (4.4.9)\n",
            "Requirement already satisfied: six>=1.5 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: comm>=0.1.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 7)) (0.2.3)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 7)) (1.8.17)\n",
            "Requirement already satisfied: ipython>=7.23.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 7)) (9.6.0)\n",
            "Requirement already satisfied: jupyter-client>=8.0.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 7)) (8.6.3)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 7)) (5.9.1)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 7)) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio>=1.4 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 7)) (1.6.0)\n",
            "Requirement already satisfied: psutil>=5.7 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 7)) (7.1.1)\n",
            "Requirement already satisfied: pyzmq>=25 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 7)) (27.1.0)\n",
            "Requirement already satisfied: tornado>=6.2 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 7)) (6.5.2)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 7)) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipywidgets->jupyter->-r requirements.txt (line 7)) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipywidgets->jupyter->-r requirements.txt (line 7)) (3.0.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.30 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-console->jupyter->-r requirements.txt (line 7)) (3.0.52)\n",
            "Requirement already satisfied: pygments in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-console->jupyter->-r requirements.txt (line 7)) (2.19.2)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 7)) (2.0.5)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 7)) (2.3.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 7)) (2.17.0)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 7)) (2.27.3)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 7)) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 7)) (65.5.0)\n",
            "Requirement already satisfied: beautifulsoup4 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 7)) (4.14.2)\n",
            "Requirement already satisfied: bleach[css]!=5.0.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 7)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 7)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 7)) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 7)) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 7)) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 7)) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 7)) (1.5.1)\n",
            "Requirement already satisfied: webencodings in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: anyio in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (4.11.0)\n",
            "Requirement already satisfied: certifi in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (1.0.9)\n",
            "Requirement already satisfied: idna in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (0.16.0)\n",
            "Requirement already satisfied: decorator in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 7)) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 7)) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 7)) (0.19.2)\n",
            "Requirement already satisfied: stack_data in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 7)) (0.6.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->-r requirements.txt (line 7)) (4.5.0)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (25.1.0)\n",
            "Requirement already satisfied: jupyter-events>=0.11.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (7.7.0)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (0.23.1)\n",
            "Requirement already satisfied: pywinpty>=2.0.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (3.0.2)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (1.9.0)\n",
            "Requirement already satisfied: babel>=2.10 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (2.17.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (4.25.1)\n",
            "Requirement already satisfied: requests>=2.31 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (2.32.5)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from nbformat>=5.7->nbconvert->jupyter->-r requirements.txt (line 7)) (2.21.2)\n",
            "Requirement already satisfied: wcwidth in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter->-r requirements.txt (line 7)) (0.2.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 7)) (2.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (25.1.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 7)) (0.8.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (0.27.1)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (4.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (6.0.3)\n",
            "Requirement already satisfied: rfc3339-validator in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (0.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (2.5.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 7)) (2.2.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 7)) (0.2.3)\n",
            "Requirement already satisfied: fqdn in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (1.5.1)\n",
            "Requirement already satisfied: isoduration in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: uri-template in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (24.11.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (2.0.0)\n",
            "Requirement already satisfied: pycparser in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 7)) (2.23)\n",
            "Requirement already satisfied: lark>=1.2.2 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from rfc3987-syntax>=1.1.0->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: arrow>=0.15.0 in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: tzdata in d:\\sarthak\\rcoem\\3rd\\projects\\ml\\synthetic media detection\\cuda-venv\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 7)) (2025.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch # used torch instead of PyTorch for felxibility and efficiency\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ensures Reproducibility (Phirse same results on every run)\n",
        "import random\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBDIRP_hAO2K"
      },
      "source": [
        "### Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBSXJKdJD5KB",
        "outputId": "e6aec868-2f0d-47a7-da23-b8f6e6dc3af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Directory: ['ai', 'nature']\n",
            "Test Directory: ['ai', 'nature']\n"
          ]
        }
      ],
      "source": [
        "# directories\n",
        "train_dir = rf\"D:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\Datasets\\GenImage\\BigGAN+VQDM\\train\"\n",
        "test_dir = rf\"D:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\Datasets\\GenImage\\BigGAN+VQDM\\val\"\n",
        "\n",
        "# Check contents of the folders\n",
        "print(\"Train Directory:\", os.listdir(train_dir))\n",
        "print(\"Test Directory:\", os.listdir(test_dir))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wvtqBK8GEQvm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 647996 | Val samples: 24000\n",
            "Classes: ['ai', 'nature']\n"
          ]
        }
      ],
      "source": [
        "# Data preprocesors and loaders\n",
        "\n",
        "# images are already resized\n",
        "train_transform = transforms.Compose([\n",
        "    # transforms.RandomCrop(224, padding=8),                                      # it keeps aspect ratio but adds slight random offset\n",
        "    transforms.RandomHorizontalFlip(p=0.5),                                     # horizontal flip with 50% probability\n",
        "    # transforms.RandomRotation(15),                                              # rotate ±15 degrees\n",
        "    # transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15),\n",
        "    # transforms.GaussianBlur(3, sigma=(0.1, 1.0)),                               # slight blur,  helps fight generator-specific noise \n",
        "    transforms.ToTensor(),                                                      # convert to tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])          # normalize to ImageNet stats\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets (Safe -> safely skips broken/missing files with datasets.ImageFolder)\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=val_transform)\n",
        "\n",
        "# Dataloaders\n",
        "batch_size = 32     # 32 images per batch\n",
        "num_workers = 1     # safe for my 12 cores CPU (preventing overload)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,    # number of parallel CPU threads preloading data while GPU trains\n",
        "    pin_memory=True,           # pin_memory=True-> speeds up CPU→GPU transfer.\n",
        "    persistent_workers=False,    # persistent_workers=True -> keeps workers alive between epochs (faster)\n",
        "    prefetch_factor=1           # Loads next batches while GPU is training (works only if num_workers > 0)\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(test_dataset, \n",
        "    batch_size=batch_size, \n",
        "    shuffle=False, \n",
        "    num_workers=num_workers, \n",
        "    pin_memory=True, \n",
        "    persistent_workers=False, \n",
        "    prefetch_factor=1\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)} | Val samples: {len(test_dataset)}\")\n",
        "print(f\"Classes: {train_dataset.classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checking loaders & resource (GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "NVIDIA GeForce GTX 1660 Ti\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking folder: C:\\Users\\sarth\\Dataset\\train\\ai\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ai:   4%|▍         | 13869/323996 [00:01<00:42, 7373.63it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m path = os.path.join(folder, f)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img:\n\u001b[32m     21\u001b[39m         img.verify()  \u001b[38;5;66;03m# Checks for corruption\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sarth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3493\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3492\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3493\u001b[39m     fp = builtins.open(filename, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3494\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Check for corrupted images in dataset folders\n",
        "FOLDERS = [\n",
        "    rf\"C:\\Users\\sarth\\Dataset\\train\\ai\",\n",
        "    rf\"C:\\Users\\sarth\\Dataset\\train\\nature\",\n",
        "    rf\"C:\\Users\\sarth\\Dataset\\val\\ai\",\n",
        "    rf\"C:\\Users\\sarth\\Dataset\\val\\nature\"\n",
        "]\n",
        "\n",
        "bad_files = []\n",
        "\n",
        "for folder in FOLDERS:\n",
        "    print(f\"Checking folder: {folder}\")\n",
        "    files = [\n",
        "        f for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\"))\n",
        "    ]\n",
        "    for f in tqdm(files, desc=os.path.basename(folder)):\n",
        "        path = os.path.join(folder, f)\n",
        "        try:\n",
        "            with Image.open(path) as img:\n",
        "                img.verify()  # Checks for corruption\n",
        "        except Exception as e:\n",
        "            bad_files.append((path, str(e)))\n",
        "\n",
        "print(f\"\\nScan complete. {len(bad_files)} bad images found.\")\n",
        "if bad_files:\n",
        "    print(\"Examples:\")\n",
        "    for bf in bad_files[:10]:\n",
        "        print(\"  \", bf[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKIw6jW7Kn7c"
      },
      "source": [
        "### Defining basic CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPScCyZhKnfF"
      },
      "outputs": [],
      "source": [
        "# SImpleCNN upgraded with explicit Grad-CAM layer and regularization\n",
        "class SimpleCNN_v2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN_v2, self).__init__()\n",
        "\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32), # stabilizes gradients\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Explicit Grad-CAM layer\n",
        "        self.last_conv = nn.Conv2d(64, 256, 3, padding=1)\n",
        "        self.bn_last   = nn.BatchNorm2d(256)\n",
        "        self.relu_last = nn.ReLU()\n",
        "        self.pool_last = nn.MaxPool2d(2)\n",
        "\n",
        "        # Global average pooling and classifier\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1) # Reduces Overfitting and enables Grad-CAM (Regularization)\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.ReLU(),           # Adds non-linearity before FC layers\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),     # Regularization (Reduces overfitting)\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.last_conv(x)\n",
        "        x = self.bn_last(x)\n",
        "        x = self.relu_last(x)\n",
        "        x = self.pool_last(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5kjnQ5ow13I"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYAg3YDHwmDe",
        "outputId": "3cc71ac1-7c5b-472b-c7d2-7fd295d25df1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sarth\\AppData\\Local\\Temp\\ipykernel_16824\\3101192458.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming from last checkpoint...\n",
            "Resumed from epoch 1 with best accuracy 0.8860\n",
            "Epoch 2/10: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  16%|█▌        | 3237/20250 [34:12<2:59:46,  1.58batch/s, loss=0.286]  \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(train_loader, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m, unit=\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tepoch:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtepoch\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# clear gradients for next batch\u001b[39;49;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1447\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1409\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1411\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1413\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1414\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1231\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1232\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1241\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1244\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1246\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1247\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1248\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\connection.py:256\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\connection.py:329\u001b[39m, in \u001b[36mPipeConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m    327\u001b[39m             _winapi.PeekNamedPipe(\u001b[38;5;28mself\u001b[39m._handle)[\u001b[32m0\u001b[39m] != \u001b[32m0\u001b[39m):\n\u001b[32m    328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\connection.py:878\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m    875\u001b[39m                 ready_objects.add(o)\n\u001b[32m    876\u001b[39m                 timeout = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m     ready_handles = \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    880\u001b[39m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[32m    881\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\connection.py:810\u001b[39m, in \u001b[36m_exhaustive_wait\u001b[39m\u001b[34m(handles, timeout)\u001b[39m\n\u001b[32m    808\u001b[39m ready = []\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m     res = _winapi.WaitForMultipleObjects(L, \u001b[38;5;28;01mFalse\u001b[39;00m, timeout)\n\u001b[32m    811\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m res == WAIT_TIMEOUT:\n\u001b[32m    812\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Setup for training with checkpointing\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN_v2().to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()      # More stable than BCELoss with raw outputs (stable binary classification)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)  # L2 Regularization\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.7) # reduces LR only if validation accuracy plateaus\n",
        "scaler = torch.amp.GradScaler(\"cuda\") # for safe scaling during backprop\n",
        "\n",
        "# Maintaing history for analysis\n",
        "history = {\"train_loss\": [], \"val_acc\": [], \"val_loss\": [], \"lr\": []}\n",
        "\n",
        "# Directory for saving checkpoints\n",
        "checkpoint_dir = \"checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"last_checkpoint.pth\")\n",
        "\n",
        "start_epoch = 0\n",
        "best_acc = 0.0\n",
        "\n",
        "# Load checkpoint if resuming\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(\"Resuming from last checkpoint...\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "    scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
        "    scaler.load_state_dict(checkpoint[\"scaler_state\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    best_acc = checkpoint.get(\"best_acc\", 0.0)\n",
        "    print(f\"Resumed from epoch {start_epoch} with best accuracy {best_acc:.4f}\")\n",
        "\n",
        "patience = 3  # for early stopping: stop after 3 epochs with no improvement\n",
        "patience_counter = 0\n",
        "epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    print(f\"Epoch {epoch+1}/{epochs}: \")\n",
        "\n",
        "    with tqdm(train_loader, desc=\"Training\", unit=\"batch\") as tepoch:\n",
        "        for images, labels in tepoch:\n",
        "            images, labels = images.to(device), labels.float().unsqueeze(1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()               # clear gradients for next batch\n",
        "            with torch.amp.autocast(\"cuda\"):    # Mixed precision forward\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward pass\n",
        "            scaler.scale(loss).backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)    # Gradients stable is not needed as we stabalised it with BatchNorm and Mixed Precision\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total, val_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss_val = criterion(outputs, labels.unsqueeze(1).float())\n",
        "            val_loss += loss_val.item()\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            correct += (preds == labels.unsqueeze(1)).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_acc = correct / total\n",
        "    val_loss /= len(val_loader)\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    if (epoch + 1) % 2 == 0 and device.type == \"cuda\":\n",
        "        print(\"Clearing unused CUDA memory to avoid fragmentation...\")\n",
        "        torch.cuda.empty_cache()        # clear unused memory every 2 epochs to avoid fragmentation\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr'] # get current learning rate\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | LR: {current_lr:.6f}\")\n",
        "\n",
        "    history[\"train_loss\"].append(avg_loss)\n",
        "    history[\"val_acc\"].append(val_acc)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"lr\"].append(current_lr)\n",
        "\n",
        "    # Proactive early stopping based on validation accuracy\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        patience_counter = 0  # reset counter\n",
        "        best_checkpoint = {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"scheduler_state\": scheduler.state_dict(),\n",
        "            \"scaler_state\": scaler.state_dict(),\n",
        "            \"best_acc\": best_acc\n",
        "        }\n",
        "        torch.save(best_checkpoint, os.path.join(checkpoint_dir, \"best_checkpoint.pth\"))\n",
        "        print(f\"Best model updated! New Val Acc: {best_acc:.4f}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"No improvement for {patience_counter}/{patience} epochs.\")\n",
        "\n",
        "    # Initiate early stop\n",
        "    if patience_counter >= patience:\n",
        "        print(\"\\nEarly stopping initiated: no improvement for 3 epochs.\")\n",
        "        torch.save(model.state_dict(), os.path.join(checkpoint_dir, \"final_best_model.pth\"))\n",
        "        print(\"Final best model saved before stopping.\")\n",
        "        break\n",
        "\n",
        "\n",
        "    # Save checkpoints\n",
        "    checkpoint = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"scheduler_state\": scheduler.state_dict(),\n",
        "        \"scaler_state\": scaler.state_dict(),\n",
        "        \"best_acc\": best_acc\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "    print(\"Initiating next epoch in 20 seconds...\")\n",
        "    time.sleep(20)  # CPU cooldown before next epoch\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nTraining complete 🔥\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache() # Clears VRAM memory cache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "model.eval()  # Make sure model is in evaluation mode\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = (outputs > 0.5).float()\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Real\", \"Fake\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix - Final Model\")\n",
        "plt.show()\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Final validation accuracy\n",
        "val_acc = (all_preds == all_labels).mean()\n",
        "print(f\"Final Validation Accuracy: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, numpy as np\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "model.eval()\n",
        "all_probs, all_preds, all_labels = [], [], []\n",
        "val_loss = 0.0\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.float().unsqueeze(1).to(device)\n",
        "        outputs = model(images)                    # logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        val_loss += loss.item() * images.size(0)\n",
        "        probs = torch.sigmoid(outputs).cpu().numpy().ravel()\n",
        "        preds = (probs > 0.5).astype(int)\n",
        "        all_probs.extend(probs.tolist())\n",
        "        all_preds.extend(preds.tolist())\n",
        "        all_labels.extend(labels.cpu().numpy().ravel().astype(int).tolist())\n",
        "\n",
        "val_loss = val_loss / len(val_dataset)\n",
        "print(f\"Val loss: {val_loss:.4f}, Val acc: {np.mean(np.array(all_preds)==np.array(all_labels)):.4f}\")\n",
        "\n",
        "# Classification report + AUC\n",
        "print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n",
        "try:\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "    print(\"ROC AUC:\", auc)\n",
        "except Exception as e:\n",
        "    print(\"ROC AUC could not be computed:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing on VQDM val dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ADM_test_dir = rf\"E:\\Datasets\\ADM\\imagenet_ai_0508_adm\\val\"\n",
        "ADM_val_dataset = datasets.ImageFolder(root=ADM_test_dir, transform=val_transform)\n",
        "ADM_val_loader = DataLoader(ADM_val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 211, in collate\n    return [\n           ^\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [3, 256, 256] at entry 0 and [3, 333, 500] at entry 16\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m correct, total = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mADM_val_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1465\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1464\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task_info[idx]\n\u001b[32m-> \u001b[39m\u001b[32m1465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1489\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\_utils.py:715\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    712\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
            "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 211, in collate\n    return [\n           ^\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\sarthak\\RCOEM\\3rd\\Projects\\ML\\Synthetic Media detection\\cuda-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [3, 256, 256] at entry 0 and [3, 333, 500] at entry 16\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN_v2().to(device)\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in ADM_val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        correct += (preds == labels.unsqueeze(1)).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "val_acc = correct / total\n",
        "scheduler.step(val_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(val_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_preds_VQDM, all_labels_VQDM = [], []\n",
        "\n",
        "model.eval()  # Make sure model is in evaluation mode\n",
        "with torch.no_grad():\n",
        "    for images, labels in ADM_val_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = (outputs > 0.5).float()\n",
        "        all_preds_VQDM.extend(preds.cpu().numpy())\n",
        "        all_labels_VQDM.extend(labels.cpu().numpy())\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(all_labels_VQDM, all_preds_VQDM)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Real\", \"Fake\"])\n",
        "disp.plot(cmap=plt.cm.Reds)\n",
        "plt.title(\"Confusion Matrix - VQDM val dataset\")\n",
        "plt.show()\n",
        "\n",
        "all_preds_VQDM = np.array(all_preds_VQDM)\n",
        "all_labels_VQDM = np.array(all_labels_VQDM)\n",
        "\n",
        "# Final validation accuracy\n",
        "val_acc = (all_preds_VQDM == all_labels_VQDM).mean()\n",
        "print(f\"Final VQDM Validation Accuracy: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_images(model, image_paths, transform, device):\n",
        "    \"\"\"\n",
        "    Display predictions for multiple input images with confidence scores.\n",
        "\n",
        "    Args:\n",
        "        model: Trained PyTorch model.\n",
        "        image_paths: List of image file paths.\n",
        "        transform: Transform pipeline (e.g., val_transform).\n",
        "        device: 'cuda' or 'cpu'.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    for i, img_path in enumerate(image_paths):\n",
        "        try:\n",
        "            # Load and preprocess image\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            img_t = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "            # Predict\n",
        "            with torch.no_grad():\n",
        "                output = model(img_t)\n",
        "                prob = torch.sigmoid(output).item()\n",
        "\n",
        "            # Classification decision\n",
        "            label = \"Fake (AI-generated)\" if prob > 0.5 else \"Real (original)\"\n",
        "            conf = prob if prob > 0.5 else 1 - prob\n",
        "\n",
        "            # Plotting\n",
        "            plt.subplot(2, (len(image_paths) + 1) // 2, i + 1)\n",
        "            plt.imshow(np.array(img))\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"{label}\\nConf: {conf:.3f}\", fontsize=10)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_paths = []\n",
        "\n",
        "evaluate_images(model, image_paths, val_transform, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cuda-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
